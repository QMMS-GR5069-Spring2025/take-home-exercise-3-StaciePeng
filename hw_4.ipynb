{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1172d67-3c7d-46ee-b557-b01900a5c4f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d53e7c-d3d9-4cfc-8e0c-c5b1e00deae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket = \"ne-gr5069\"\n",
    "\n",
    "def load_s3_csv(key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(obj['Body'])\n",
    "\n",
    "# Replace with actual paths from your S3 bucket\n",
    "pitstops_df = load_s3_csv(\"raw/pit_stops.csv\")\n",
    "laptimes_df = load_s3_csv(\"raw/lap_times.csv\")\n",
    "results_df = load_s3_csv(\"raw/results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618c01ad-908c-4acc-b5d5-960723048051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pitstop_counts = pitstops_df.groupby(['raceId', 'driverId']).size().reset_index(name='pitstop_count')\n",
    "\n",
    "# Join with results (to get positionOrder as label)\n",
    "df = pd.merge(results_df, pitstop_counts, on=['raceId', 'driverId'], how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3bec54-9322-472a-8e6e-57aa1c4dab80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b33398-1296-42ab-850a-b9f0cf7082b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = ['pitstop_count', 'grid', 'laps', 'fastestLap', 'rank']\n",
    "\n",
    "# Drop rows with any missing values in selected features\n",
    "df_model = df[feature_cols + ['positionOrder']].dropna()\n",
    "\n",
    "# Define X and y\n",
    "X = df_model[feature_cols]\n",
    "y = df_model['positionOrder'].apply(lambda x: 1 if x <= 3 else 0) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb86b17-a974-439e-bcac-5f21dbebdf2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "\n",
    "# Convert non-numeric columns to numeric, setting errors='coerce' to convert non-numeric values to NaN\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f250a684-3db4-4ac5-966a-fa25297fafa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_pitstop_model(experimentID, run_name, params, X_train, X_test, y_train, y_test):\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    import mlflow.sklearn\n",
    "    import seaborn as sns\n",
    "    import tempfile\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "    with mlflow.start_run(experiment_id=experimentID, run_name=run_name) as run:\n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(X_train_imputed, y_train)\n",
    "        predictions = model.predict(X_test_imputed)\n",
    "\n",
    "        # Log model & parameters\n",
    "        mlflow.sklearn.log_model(model, \"pitstop-model\")\n",
    "        [mlflow.log_param(k, v) for k, v in params.items()]\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy_score(y_test, predictions))\n",
    "        mlflow.log_metric(\"precision\", precision_score(y_test, predictions))\n",
    "        mlflow.log_metric(\"recall\", recall_score(y_test, predictions))\n",
    "        mlflow.log_metric(\"f1_score\", f1_score(y_test, predictions))\n",
    "\n",
    "        # Confusion matrix plot\n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", ax=ax)\n",
    "        ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "        temp = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\")\n",
    "        plt.savefig(temp.name)\n",
    "        mlflow.log_artifact(temp.name, \"confusion_matrix.png\")\n",
    "        display(fig)\n",
    "\n",
    "        return run.info.run_uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6d513da-9d92-4ca6-99d6-ed02fcb3647f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "experimentID = mlflow.set_experiment(\"/Users/yp2728@columbia.edu/F1_prediction_model\").experiment_id\n",
    "\n",
    "param_grid = [\n",
    "    {\"n_estimators\": 60, \"max_depth\": 4, \"random_state\":12},\n",
    "    {\"n_estimators\": 120, \"max_depth\": 6, \"random_state\": 42},\n",
    "    {\"n_estimators\": 180, \"max_depth\": 8, \"random_state\": 42},\n",
    "    {\"n_estimators\": 110, \"max_depth\": None, \"random_state\": 42},\n",
    "    {\"n_estimators\": 220, \"max_depth\": 6, \"random_state\": 42},\n",
    "    {\"n_estimators\": 320, \"max_depth\": 12, \"random_state\": 42},\n",
    "    {\"n_estimators\": 90, \"max_depth\": 5, \"random_state\": 42},\n",
    "    {\"n_estimators\": 130, \"max_depth\": 7, \"random_state\": 42},\n",
    "    {\"n_estimators\": 270, \"max_depth\": 9, \"random_state\": 42},\n",
    "    {\"n_estimators\": 110, \"max_depth\": 14, \"random_state\": 42},\n",
    "]\n",
    "\n",
    "for i, params in enumerate(param_grid):\n",
    "    log_pitstop_model(experimentID, f\"Pitstop Run {i+1}\", params, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a87629b-b6f6-4b0d-b39c-69ad0958d603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For this assignment, I selected Pitstop Run 5 as the best model based on its overall performance across key evaluation metrics. While several runs had similar accuracy scores, Pitstop Run 5 achieved the highest F1 score of 0.674967, indicating a strong balance between precision and recall. This balance is especially important in our task of predicting podium finishes, where both false positives and false negatives can be problematic. The model also demonstrated solid precision and recall scores of 0.674967 each, reinforcing its consistency. Although Pitstop Run 1 had a slightly higher F1 score, it used a different random state and had lower precision, which may indicate variability in results. Pitstop Run 5, with hyperparameters of 220 estimators and a maximum depth of 6, showed stable and reliable performance, making it the most suitable model for this classification task."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hw_4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
